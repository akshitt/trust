

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Strategies &mdash; TRUST v0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Utilities" href="trust.utils.html" />
    <link rel="prev" title="TRUST" href="modules.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> TRUST
          

          
            
            <img src="../_static/trust_logo.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">TRUST</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Strategies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#module-trust.strategies.smi">SMI</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-trust.strategies.scg">SCG</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-trust.strategies.scmi">SCMI</a></li>
<li class="toctree-l3"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="trust.utils.html">Utilities</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">TRUST</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="modules.html">TRUST</a> &raquo;</li>
        
      <li>Strategies</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/modules/trust.strategies.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="strategies">
<h1>Strategies<a class="headerlink" href="#strategies" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-trust.strategies.smi">
<span id="smi"></span><h2>SMI<a class="headerlink" href="#module-trust.strategies.smi" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="trust.strategies.smi.SMI">
<em class="property">class </em><code class="sig-prename descclassname">trust.strategies.smi.</code><code class="sig-name descname">SMI</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">query_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/trust/strategies/smi.html#SMI"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#trust.strategies.smi.SMI" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">trust.strategies.strategy.Strategy</span></code></p>
<p>This strategy implements the Submodular Mutual Information (SMI) selection paradigm discuss in the paper
SIMILAR: Submodular Information Measures Based Active Learning In Realistic Scenarios <a class="footnote-reference brackets" href="#kothawade2021similar" id="id1">1</a>. In this selection
paradigm, points from the unlabeled dataset are chosen in such a way that the submodular mutual information
between this set of points and a provided query set is maximized. Doing so allows a practitioner to select
points from an unlabeled set that are SIMILAR to points that they have provided in a active learning query.</p>
<p>These submodular mutual information functions rely on formulating embeddings for the points in the query set
and the unlabeled set. Once these embeddings are formed, one or more similarity kernels (depending on the
SMI function used) are formed from these embeddings based on a similarity metric. Once these similarity kernels
are formed, they are used in computing the value of each submodular mutual information function. Hence, common
techniques for submodular maximization subject to a cardinality constraint can be used, such as the naive greedy
algorithm, the lazy greedy algorithm, and so forth.</p>
<p>In this framework, we set the cardinality constraint to be the active learning selection budget; hence, a list of
indices with a total length less than or equal to this cardinality constraint will be returned. Depending on the
maximization configuration, one can ensure that the length of this list will be equal to the cardinality constraint.</p>
<p>Currently, five submodular mutual information functions are implemented: fl1mi, fl2mi, gcmi, logdetmi, and com. Each
function is obtained by applying the definition of a submodular mutual information function using common submodular
functions. Facility Location Mutual Information (fl1mi) models pairwise similarities of points in the query set to
points in the unlabeled dataset AND pairwise similarities of points within the unlabeled datasets. Another variant of
Facility Location Mutual Information (fl2mi) models pairwise similarities of points in the query set to points in
the unlabeled dataset ONLY. Graph Cut Mutual Information (gcmi), Log-Determinant Mutual Information (logdetmi), and
Concave-Over-Modular Mutual Information (com) are all obtained by applying the usual submodular function under this
definition. For more information-theoretic discussion, consider referring to the paper Submodular Combinatorial
Information Measures with Applications in Machine Learning <a class="footnote-reference brackets" href="#iyer2021submodular" id="id2">2</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled dataset to be used in this strategy. For the purposes of selection, the labeled dataset is not used,
but it is provided to fit the common framework of the Strategy superclass.</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled dataset to be used in this strategy. It is used in the selection process as described above.
Importantly, the unlabeled dataset must return only a data Tensor; if indexing the unlabeled dataset returns a tuple of
more than one component, unexpected behavior will most likely occur.</p></li>
<li><p><strong>query_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The query dataset to be used in this strategy. It is used in the selection process as described above. Notably,
the query dataset should be labeled; hence, indexing the query dataset should return a data/label pair. This is
done in this fashion to allow for gradient embeddings.</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The neural network model to use for embeddings and predictions. Notably, all embeddings typically come from extracted
features from this network or from gradient embeddings based on the loss, which can be based on hypothesized gradients
or on true gradients (depending on the availability of the label).</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – The number of classes being predicted by the neural network.</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <dl class="simple">
<dt>A dictionary containing many configurable settings for this strategy. Each key-value pair is described below:</dt><dd><dl class="simple">
<dt>’batch_size’: int</dt><dd><p>The batch size used internally for torch.utils.data.DataLoader objects. Default: 1</p>
</dd>
<dt>’device’: string</dt><dd><p>The device to be used for computation. PyTorch constructs are transferred to this device. Usually is one
of ‘cuda’ or ‘cpu’. Default: ‘cuda’ if a CUDA-enabled device is available; otherwise, ‘cpu’</p>
</dd>
<dt>’loss’: function</dt><dd><p>The loss function to be used in computations. Default: torch.nn.functional.cross_entropy</p>
</dd>
<dt>’smi_function’: string</dt><dd><p>The submodular mutual information function to use in optimization. Must be one of ‘fl1mi’, ‘fl2mi’, ‘gcmi’,
‘logdetmi’, ‘com’. REQUIRED</p>
</dd>
<dt>’optimizer’: string</dt><dd><p>The optimizer to use for submodular maximization. Can be one of ‘NaiveGreedy’, ‘StochasticGreedy’,
‘LazyGreedy’ and ‘LazierThanLazyGreedy’. Default: ‘NaiveGreedy’</p>
</dd>
<dt>’metric’: string</dt><dd><p>The similarity metric to use for similarity kernel computation. This can be either ‘cosine’ or ‘euclidean’.
Default: ‘cosine’</p>
</dd>
<dt>’eta’: float</dt><dd><p>A magnification constant that is used in all but gcmi. It is used as a value of query-relevance vs diversity
trade-off. Increasing eta tends to increase query-relevance while reducing query-coverage and diversity.
Default: 1</p>
</dd>
<dt>’embedding_type’: string</dt><dd><p>The type of embedding to compute for similarity kernel computation. This can be either ‘gradients’ or
‘features’. Default: ‘gradients’</p>
</dd>
<dt>’gradType’: string</dt><dd><p>When ‘embedding_type’ is ‘gradients’, this defines the type of gradient to use. ‘bias’ creates gradients from
the loss function with respect to the biases outputted by the model. ‘linear’ creates gradients from the
loss function with respect to the last linear layer features. ‘bias_linear’ creates gradients from the
loss function using both. Default: ‘bias_linear’</p>
</dd>
<dt>’layer_name’: string</dt><dd><p>When ‘embedding_type’ is ‘features’, this defines the layer within the neural network that is used to extract
feature embeddings. Namely, this argument must be the name of a module used in the forward() computation of
the model. Default: ‘avgpool’</p>
</dd>
<dt>’stopIfZeroGain’: bool</dt><dd><p>Controls if the optimizer should cease maximization if there is zero gain in the submodular objective.
Default: False</p>
</dd>
<dt>’stopIfNegativeGain’: bool</dt><dd><p>Controls if the optimizer should cease maximization if there is negative gain in the submodular objective.
Default: False</p>
</dd>
<dt>’verbose’: bool</dt><dd><p>Gives a more verbose output when calling select() when True. Default: False</p>
</dd>
</dl>
</dd>
</dl>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="trust.strategies.smi.SMI.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/trust/strategies/smi.html#SMI.select"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#trust.strategies.smi.SMI.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects a set of points from the unlabeled dataset to label based on this strategy’s methodology.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>budget</strong> (<em>int</em>) – Number of points to choose from the unlabeled dataset</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>chosen</strong> – List of selected data point indices with respect to the unlabeled dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-trust.strategies.scg">
<span id="scg"></span><h2>SCG<a class="headerlink" href="#module-trust.strategies.scg" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="trust.strategies.scg.SCG">
<em class="property">class </em><code class="sig-prename descclassname">trust.strategies.scg.</code><code class="sig-name descname">SCG</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">private_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/trust/strategies/scg.html#SCG"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#trust.strategies.scg.SCG" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">trust.strategies.strategy.Strategy</span></code></p>
<p>This strategy implements the Submodular Conditional Gain (SCG) selection paradigm discuss in the paper
SIMILAR: Submodular Information Measures Based Active Learning In Realistic Scenarios <a class="footnote-reference brackets" href="#kothawade2021similar" id="id3">1</a>. In this selection
paradigm, points from the unlabeled dataset are chosen in such a way that the submodular conditional gain
between this set of points and a provided private set is maximized. Doing so allows a practitioner to select
points from an unlabeled set that are dissimilar to points provided in the private set.</p>
<p>These submodular conditional gain functions rely on formulating embeddings for the points in the unlabeled set
and the private set. Once these embeddings are formed, similarity kernels are formed from these
embeddings based on a similarity metric. Once these similarity kernels are formed, they are used in computing the value
of each submodular conditional gain function. Hence, common techniques for submodular maximization subject to a
cardinality constraint can be used, such as the naive greedy algorithm, the lazy greedy algorithm, and so forth.</p>
<p>In this framework, we set the cardinality constraint to be the active learning selection budget; hence, a list of
indices with a total length less than or equal to this cardinality constraint will be returned. Depending on the
maximization configuration, one can ensure that the length of this list will be equal to the cardinality constraint.</p>
<p>Currently, two submodular conditional gain functions are implemented: ‘flcg’, ‘gccg’, and ‘logdetcg’. Each
function is obtained by applying the definition of a submodular conditional gain function using common
submodular functions. For more information-theoretic discussion, consider referring to the paper Submodular Combinatorial
Information Measures with Applications in Machine Learning <a class="footnote-reference brackets" href="#iyer2021submodular" id="id4">2</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled dataset to be used in this strategy. For the purposes of selection, the labeled dataset is not used,
but it is provided to fit the common framework of the Strategy superclass.</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled dataset to be used in this strategy. It is used in the selection process as described above.
Importantly, the unlabeled dataset must return only a data Tensor; if indexing the unlabeled dataset returns a tuple of
more than one component, unexpected behavior will most likely occur.</p></li>
<li><p><strong>private_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The private dataset to be used in this strategy. It is used in the selection process as described above. Notably,
the private dataset should be labeled; hence, indexing the query dataset should return a data/label pair. This is
done in this fashion to allow for gradient embeddings.</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The neural network model to use for embeddings and predictions. Notably, all embeddings typically come from extracted
features from this network or from gradient embeddings based on the loss, which can be based on hypothesized gradients
or on true gradients (depending on the availability of the label).</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – The number of classes being predicted by the neural network.</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <dl class="simple">
<dt>A dictionary containing many configurable settings for this strategy. Each key-value pair is described below:</dt><dd><dl class="simple">
<dt>’batch_size’: int</dt><dd><p>The batch size used internally for torch.utils.data.DataLoader objects. Default: 1</p>
</dd>
<dt>’device’: string</dt><dd><p>The device to be used for computation. PyTorch constructs are transferred to this device. Usually is one
of ‘cuda’ or ‘cpu’. Default: ‘cuda’ if a CUDA-enabled device is available; otherwise, ‘cpu’</p>
</dd>
<dt>’loss’: function</dt><dd><p>The loss function to be used in computations. Default: torch.nn.functional.cross_entropy</p>
</dd>
<dt>’scg_function’: string</dt><dd><p>The submodular mutual information function to use in optimization. Must be one of ‘flcmi’ or ‘logdetcmi’.
REQUIRED</p>
</dd>
<dt>’optimizer’: string</dt><dd><p>The optimizer to use for submodular maximization. Can be one of ‘NaiveGreedy’, ‘StochasticGreedy’,
‘LazyGreedy’ and ‘LazierThanLazyGreedy’. Default: ‘NaiveGreedy’</p>
</dd>
<dt>’metric’: string</dt><dd><p>The similarity metric to use for similarity kernel computation. This can be either ‘cosine’ or ‘euclidean’.
Default: ‘cosine’</p>
</dd>
<dt>’nu’: float</dt><dd><p>A parameter that governs the hardness of the privacy constraint. Default: 1.</p>
</dd>
<dt>’embedding_type’: string</dt><dd><p>The type of embedding to compute for similarity kernel computation. This can be either ‘gradients’ or
‘features’. Default: ‘gradients’</p>
</dd>
<dt>’gradType’: string</dt><dd><p>When ‘embedding_type’ is ‘gradients’, this defines the type of gradient to use. ‘bias’ creates gradients from
the loss function with respect to the biases outputted by the model. ‘linear’ creates gradients from the
loss function with respect to the last linear layer features. ‘bias_linear’ creates gradients from the
loss function using both. Default: ‘bias_linear’</p>
</dd>
<dt>’layer_name’: string</dt><dd><p>When ‘embedding_type’ is ‘features’, this defines the layer within the neural network that is used to extract
feature embeddings. Namely, this argument must be the name of a module used in the forward() computation of
the model. Default: ‘avgpool’</p>
</dd>
<dt>’stopIfZeroGain’: bool</dt><dd><p>Controls if the optimizer should cease maximization if there is zero gain in the submodular objective.
Default: False</p>
</dd>
<dt>’stopIfNegativeGain’: bool</dt><dd><p>Controls if the optimizer should cease maximization if there is negative gain in the submodular objective.
Default: False</p>
</dd>
<dt>’verbose’: bool</dt><dd><p>Gives a more verbose output when calling select() when True. Default: False</p>
</dd>
</dl>
</dd>
</dl>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="trust.strategies.scg.SCG.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/trust/strategies/scg.html#SCG.select"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#trust.strategies.scg.SCG.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects a set of points from the unlabeled dataset to label based on this strategy’s methodology.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>budget</strong> (<em>int</em>) – Number of points to choose from the unlabeled dataset</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>chosen</strong> – List of selected data point indices with respect to the unlabeled dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-trust.strategies.scmi">
<span id="scmi"></span><h2>SCMI<a class="headerlink" href="#module-trust.strategies.scmi" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="trust.strategies.scmi.SCMI">
<em class="property">class </em><code class="sig-prename descclassname">trust.strategies.scmi.</code><code class="sig-name descname">SCMI</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">query_dataset</span></em>, <em class="sig-param"><span class="n">private_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/trust/strategies/scmi.html#SCMI"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#trust.strategies.scmi.SCMI" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">trust.strategies.strategy.Strategy</span></code></p>
<p>This strategy implements the Submodular Conditional Mutual Information (SCMI) selection paradigm discuss in the paper
SIMILAR: Submodular Information Measures Based Active Learning In Realistic Scenarios <a class="footnote-reference brackets" href="#kothawade2021similar" id="id5">1</a>. In this selection
paradigm, points from the unlabeled dataset are chosen in such a way that the submodular conditional mutual information
between this set of points and a provided query set is maximized, conditioned on a private dataset.
Doing so allows a practitioner to select points from an unlabeled set that are SIMILAR to points that they have
provided in the query set while being dissimilar to points provided in the private set.</p>
<p>These submodular conditional mutual information functions rely on formulating embeddings for the points in the query set,
the unlabeled set, and the private set. Once these embeddings are formed, similarity kernels are formed from these
embeddings based on a similarity metric. Once these similarity kernels are formed, they are used in computing the value
of each submodular conditional mutual information function. Hence, common techniques for submodular maximization
subject to a cardinality constraint can be used, such as the naive greedy algorithm, the lazy greedy algorithm, and so forth.</p>
<p>In this framework, we set the cardinality constraint to be the active learning selection budget; hence, a list of
indices with a total length less than or equal to this cardinality constraint will be returned. Depending on the
maximization configuration, one can ensure that the length of this list will be equal to the cardinality constraint.</p>
<p>Currently, two submodular conditional mutual information functions are implemented: ‘flcmi’ and ‘logdetcmi’. Each
function is obtained by applying the definition of a submodular conditional mutual information function using common
submodular functions. For more information-theoretic discussion, consider referring to the paper Submodular Combinatorial
Information Measures with Applications in Machine Learning <a class="footnote-reference brackets" href="#iyer2021submodular" id="id6">2</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled dataset to be used in this strategy. For the purposes of selection, the labeled dataset is not used,
but it is provided to fit the common framework of the Strategy superclass.</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled dataset to be used in this strategy. It is used in the selection process as described above.
Importantly, the unlabeled dataset must return only a data Tensor; if indexing the unlabeled dataset returns a tuple of
more than one component, unexpected behavior will most likely occur.</p></li>
<li><p><strong>query_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The query dataset to be used in this strategy. It is used in the selection process as described above. Notably,
the query dataset should be labeled; hence, indexing the query dataset should return a data/label pair. This is
done in this fashion to allow for gradient embeddings.</p></li>
<li><p><strong>private_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The private dataset to be used in this strategy. It is used in the selection process as described above. Notably,
the private dataset should be labeled; hence, indexing the query dataset should return a data/label pair. This is
done in this fashion to allow for gradient embeddings.</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The neural network model to use for embeddings and predictions. Notably, all embeddings typically come from extracted
features from this network or from gradient embeddings based on the loss, which can be based on hypothesized gradients
or on true gradients (depending on the availability of the label).</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – The number of classes being predicted by the neural network.</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <dl class="simple">
<dt>A dictionary containing many configurable settings for this strategy. Each key-value pair is described below:</dt><dd><dl class="simple">
<dt>’batch_size’: int</dt><dd><p>The batch size used internally for torch.utils.data.DataLoader objects. Default: 1</p>
</dd>
<dt>’device’: string</dt><dd><p>The device to be used for computation. PyTorch constructs are transferred to this device. Usually is one
of ‘cuda’ or ‘cpu’. Default: ‘cuda’ if a CUDA-enabled device is available; otherwise, ‘cpu’</p>
</dd>
<dt>’loss’: function</dt><dd><p>The loss function to be used in computations. Default: torch.nn.functional.cross_entropy</p>
</dd>
<dt>’scmi_function’: string</dt><dd><p>The submodular mutual information function to use in optimization. Must be one of ‘flcmi’ or ‘logdetcmi’.
REQUIRED</p>
</dd>
<dt>’optimizer’: string</dt><dd><p>The optimizer to use for submodular maximization. Can be one of ‘NaiveGreedy’, ‘StochasticGreedy’,
‘LazyGreedy’ and ‘LazierThanLazyGreedy’. Default: ‘NaiveGreedy’</p>
</dd>
<dt>’metric’: string</dt><dd><p>The similarity metric to use for similarity kernel computation. This can be either ‘cosine’ or ‘euclidean’.
Default: ‘cosine’</p>
</dd>
<dt>’eta’: float</dt><dd><p>A magnification constant that is used in all but gcmi. It is used as a value of query-relevance vs diversity
trade-off. Increasing eta tends to increase query-relevance while reducing query-coverage and diversity.
Default: 1</p>
</dd>
<dt>’nu’: float</dt><dd><p>A parameter that governs the hardness of the privacy constraint. Default: 1.</p>
</dd>
<dt>’embedding_type’: string</dt><dd><p>The type of embedding to compute for similarity kernel computation. This can be either ‘gradients’ or
‘features’. Default: ‘gradients’</p>
</dd>
<dt>’gradType’: string</dt><dd><p>When ‘embedding_type’ is ‘gradients’, this defines the type of gradient to use. ‘bias’ creates gradients from
the loss function with respect to the biases outputted by the model. ‘linear’ creates gradients from the
loss function with respect to the last linear layer features. ‘bias_linear’ creates gradients from the
loss function using both. Default: ‘bias_linear’</p>
</dd>
<dt>’layer_name’: string</dt><dd><p>When ‘embedding_type’ is ‘features’, this defines the layer within the neural network that is used to extract
feature embeddings. Namely, this argument must be the name of a module used in the forward() computation of
the model. Default: ‘avgpool’</p>
</dd>
<dt>’stopIfZeroGain’: bool</dt><dd><p>Controls if the optimizer should cease maximization if there is zero gain in the submodular objective.
Default: False</p>
</dd>
<dt>’stopIfNegativeGain’: bool</dt><dd><p>Controls if the optimizer should cease maximization if there is negative gain in the submodular objective.
Default: False</p>
</dd>
<dt>’verbose’: bool</dt><dd><p>Gives a more verbose output when calling select() when True. Default: False</p>
</dd>
</dl>
</dd>
</dl>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="trust.strategies.scmi.SCMI.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/trust/strategies/scmi.html#SCMI.select"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#trust.strategies.scmi.SCMI.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects a set of points from the unlabeled dataset to label based on this strategy’s methodology.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>budget</strong> (<em>int</em>) – Number of points to choose from the unlabeled dataset</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>chosen</strong> – List of selected data point indices with respect to the unlabeled dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p><dl class="footnote brackets">
<dt class="label" id="kothawade2021similar"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id3">2</a>,<a href="#id5">3</a>)</span></dt>
<dd><p>Suraj Kothawade, Nathan Beck, Krishnateja Killamsetty, and Rishabh Iyer. Similar: submodular information measures based active learning in realistic scenarios. <em>arXiv preprint arXiv:2107.00717</em>, 2021.</p>
</dd>
<dt class="label" id="iyer2021submodular"><span class="brackets">2</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id4">2</a>,<a href="#id6">3</a>)</span></dt>
<dd><p>Rishabh Iyer, Ninad Khargoankar, Jeff Bilmes, and Himanshu Asanani. Submodular combinatorial information measures with applications in machine learning. In <em>Algorithmic Learning Theory</em>, 722–754. PMLR, 2021.</p>
</dd>
</dl>
</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="trust.utils.html" class="btn btn-neutral float-right" title="Utilities" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="modules.html" class="btn btn-neutral" title="TRUST" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, DECILE.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'v0.1',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="../_static/language_data.js"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>